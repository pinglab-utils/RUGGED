from .base_llm import BaseLLM
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface.llms import HuggingFacePipeline
from langchain.chains import ConversationChain
from config import HF_MODEL
# source: https://python.langchain.com/docs/integrations/llms/huggingface_pipelines/

class HuggingFaceLLM(BaseLLM):
    def __init__(self, device: int = 0, max_new_tokens: int = 50):
        self.tokenizer = AutoTokenizer.from_pretrained(HF_MODEL)
        self.model = AutoModelForCausalLM.from_pretrained(HF_MODEL, device_map="auto" if device == "auto" else None)
        self.pipeline = pipeline("text-generation", model=self.model, tokenizer=self.tokenizer, device=device, max_new_tokens=max_new_tokens)

        # Wrap the HuggingFace pipeline with LangChain's HuggingFacePipeline class
        self.llm = ConversationChain(llm=HuggingFacePipeline(pipeline=self.pipeline))
        self.index = 0

    def invoke(self, prompt: str) -> str:
        """
        Invokes the Hugging Face model to generate a response for the given prompt.
        :param prompt: The input prompt to process.
        :return: The model's response as a string.
        """
        self.index += 2
        return self.llm.invoke(prompt)

    def get_response(self):
        """
        Retrieves the response generated by the Hugging Face model.
        :return: The response content as a string.
        """
        # Assuming LangChain's ConversationChain maintains chat memory
        message = self.llm.memory.chat_memory.messages[self.index - 1].content
        return message
